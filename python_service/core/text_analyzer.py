# =====================================================================
# text_analyzer.py - AI å¤šæ¨¡æ…‹å¯ä¿¡åº¦åˆ†æï¼ˆè·¨ç‰ˆæœ¬ 3.8~3.13 é€šç”¨ï¼‰
# å®Œå…¨ç§»é™¤ sentence-transformers / torch ä¾è³´
# =====================================================================

import re
import logging
import numpy as np

try:
    import jieba
except Exception:
    jieba = None

try:
    import requests
except Exception:
    requests = None

from urllib.parse import urlparse

try:
    from bs4 import BeautifulSoup
except Exception:
    BeautifulSoup = None

from core.model_loader import get_model

# âœ… åŒ¯å…¥ Gemini åŠŸèƒ½ï¼ˆè‡ªå‹•å« Vision / Combinedï¼‰
try:
    from core.gemini_client import (
        ask_gemini_vision_score,
        ask_gemini_combined,
    )
except Exception:
    ask_gemini_vision_score = None
    ask_gemini_combined = None


# ==========================================================
# ğŸ”§ è‡ªè¨‚ç°¡æ˜“èªæ„åˆ†æ•¸ï¼ˆå–ä»£ SentenceTransformerï¼‰
# ==========================================================
def simple_embedding_score(text: str) -> float:
    """
    å–ä»£ SentenceTransformer çš„ã€Œç°¡æ˜“æ–‡å­—èªæ„å¼·åº¦åˆ†æ•¸ã€ï¼Œ
    ä¸ä¾è³´ torch / transformersï¼Œå¯è·¨ Python 3.8~3.13 ä½¿ç”¨ã€‚
    """
    if not text:
        return 0.0

    length = len(text)
    unique_chars = len(set(text))

    # å­—å…ƒå¤šæ¨£æ€§ + é•·åº¦ï¼Œç²—ç•¥ç•¶æˆèªæ„ã€Œè±å¯Œåº¦ã€
    score = (unique_chars / max(length, 1)) * 0.8
    score += min(length / 500, 0.2)

    return float(round(score, 4))


# ==========================================================
# ğŸ§  ä¸»åˆ†æå‡½å¼ï¼ˆè‡ªå‹•åˆ¤æ–·æ¨¡æ…‹ï¼‰
# ==========================================================
def analyze_text(text: str, image_path: str = None) -> dict:
    """
    æ™ºæ…§åˆ†ææµç¨‹ï¼š
    1ï¸âƒ£ è‡ªå‹•åˆ¤æ–·è¼¸å…¥æ˜¯ç¶²å€ / åœ–ç‰‡ / ç´”æ–‡å­— / æ··åˆ
    2ï¸âƒ£ è‹¥ç‚ºç¶²å€ï¼Œè‡ªå‹•çˆ¬å–æ–°èæ–‡å­—å…§å®¹
    3ï¸âƒ£ LightGBM æ¨¡å‹é æ¸¬æ–‡å­—å¯ä¿¡åº¦
    4ï¸âƒ£ è‹¥æœ‰åœ–ç‰‡ï¼Œå‘¼å« Gemini é€²è¡Œåœ–ç‰‡ / åœ–æ–‡åˆ†æ
    5ï¸âƒ£ åˆ†æ•¸èåˆ + ç”Ÿæˆæ‘˜è¦èˆ‡åˆ†é¡
    """
    model = get_model()
    if model is None:
        raise RuntimeError("âŒ LightGBM æ¨¡å‹æœªè¼‰å…¥")

    text = (text or "").strip()
    if not text and not image_path:
        return {"error": "æœªè¼¸å…¥ä»»ä½•å…§å®¹"}

    # ======================================================
    # Step 1ï¸âƒ£ åˆ¤æ–·å…§å®¹é¡å‹
    # ======================================================
    if re.match(r"^https?://", text):
        mode = "ç¶²å€"
    elif image_path and text:
        mode = "æ··åˆ"
    elif image_path:
        mode = "åœ–ç‰‡"
    else:
        mode = "æ–‡å­—"

    logging.info(f"ğŸ§© åµæ¸¬è¼¸å…¥æ¨¡å¼ï¼š{mode}")

    # ======================================================
    # Step 2ï¸âƒ£ è‹¥æ˜¯ç¶²å€ â†’ çˆ¬å–æ–‡å­—
    # ======================================================
    if mode == "ç¶²å€" and requests and BeautifulSoup:
        try:
            logging.info(f"ğŸŒ åµæ¸¬åˆ°ç¶²å€ï¼Œé–‹å§‹çˆ¬å–å…§å®¹ï¼š{text}")
            text = fetch_url_text(text)
            logging.info(f"âœ… ç¶²é æ–‡å­—æ“·å–å®Œæˆï¼ˆé•·åº¦ {len(text)}ï¼‰")
        except Exception as e:
            logging.warning(f"âš ï¸ ç¶²é çˆ¬å–å¤±æ•—ï¼š{e}")
            text = f"ï¼ˆç„¡æ³•æ“·å–ç¶²é å…§å®¹ï¼‰{text}"

    # ======================================================
    # Step 3ï¸âƒ£ LightGBM æ–‡å­—é æ¸¬
    # ======================================================
    features = extract_features(text)
    score, level = predict_credibility(features)
    summary = generate_summary(text, score, level)

    # ======================================================
    # Step 4ï¸âƒ£ åœ–ç‰‡ / åœ–æ–‡åˆ†æï¼ˆGeminiï¼‰
    # ======================================================
    vision_result = None
    final_score = score  # é è¨­ç‚ºæ–‡å­—å¯ä¿¡åº¦

    try:
        # ç´”åœ–ç‰‡æ¨¡å¼
        if mode == "åœ–ç‰‡" and ask_gemini_vision_score and image_path:
            logging.info("ğŸ–¼ï¸ é€²è¡Œ Gemini å–®å¼µåœ–ç‰‡åˆ†æ")
            vision_result = ask_gemini_vision_score(
                "è«‹åˆ¤æ–·é€™å¼µåœ–ç‰‡æ˜¯å¦çœŸå¯¦æˆ–è¢«ç¯¡æ”¹ï¼Œä¸¦ç°¡çŸ­èªªæ˜ç†ç”±ã€‚",
                image_path,
            )
            v_score = float(vision_result.get("score", score))
            final_score = v_score
            summary += f"\nğŸ“¸ åœ–ç‰‡åˆ†æå¯ä¿¡åº¦ï¼š{v_score:.2f}"

        # åœ–æ–‡æ··åˆæ¨¡å¼
        elif mode == "æ··åˆ" and ask_gemini_combined and image_path:
            logging.info("ğŸ§  é€²è¡Œ Gemini åœ–æ–‡ç¶œåˆåˆ†æ")
            combined_result = ask_gemini_combined(
                "é€™æ˜¯ä¸€å‰‡åœ–æ–‡å…§å®¹ï¼Œè«‹ç¶œåˆè©•ä¼°çœŸå¯¦æ€§èˆ‡ä¸€è‡´æ€§ã€‚",
                image_path,
            )

            vision_result = combined_result
            v_score = float(combined_result.get("score", score))
            final_score = round((score + v_score) / 2, 4)
            summary += f"\nğŸ§© åœ–æ–‡ç¶œåˆåˆ†æï¼šåŠ æ¬Šå¾Œå¯ä¿¡åº¦ {final_score:.2f}ã€‚"

    except Exception as e:
        logging.warning(f"âš ï¸ Gemini åœ–åƒåˆ†æå¤±æ•—ï¼š{e}")

    # ======================================================
    # Step 5ï¸âƒ£ å›å‚³çµæœ
    # ======================================================
    return {
        "mode": mode,
        "score": round(final_score, 4),
        "level": convert_score_to_label(final_score),
        "summary": summary,
        "features_used": features,
        "keywords": extract_keywords(text),
        "category": guess_category(text),
        "text_preview": text[:120],
        "has_media": bool(image_path),
        "vision_result": vision_result,
        "status": "ok",
    }


# ==========================================================
# ğŸ” LightGBM é æ¸¬
# ==========================================================
def predict_credibility(features):
    model = get_model()
    try:
        features_array = np.array([features], dtype=float)
        raw_score = model.predict(features_array)

        # å¤šåˆ†é¡ï¼šå›å‚³æ©Ÿç‡å‘é‡
        if len(raw_score.shape) > 1 and raw_score.shape[1] > 1:
            class_probs = raw_score[0]
            top_idx = int(np.argmax(class_probs))
            score = float(class_probs[top_idx])
            label_map = ["æ¥µä½", "ä½", "ä¸­", "é«˜", "æ¥µé«˜", "æœªçŸ¥"]
            level = label_map[top_idx] if top_idx < len(label_map) else "æœªçŸ¥"
        else:
            # å–®è¼¸å‡ºï¼šè¦–ç‚º 0~1 åˆ†æ•¸
            score = float(np.ravel(raw_score)[0])
            score = float(np.clip(score, 0.0, 1.0))
            level = convert_score_to_label(score)
    except Exception as e:
        logging.error(f"âš ï¸ æ¨¡å‹é æ¸¬å¤±æ•—ï¼š{e}")
        score, level = 0.0, "æœªçŸ¥"

    return score, level


# ==========================================================
# ğŸŒ è‡ªå‹•çˆ¬å–æ–°èæ–‡å­—
# ==========================================================
def fetch_url_text(url: str) -> str:
    if not (requests and BeautifulSoup):
        raise RuntimeError("requests æˆ– BeautifulSoup æœªå®‰è£ï¼Œç„¡æ³•çˆ¬å–ç¶²é ã€‚")

    headers = {"User-Agent": "Mozilla/5.0 (compatible; TruthLiesDetector/1.0)"}
    resp = requests.get(url, headers=headers, timeout=8)
    resp.encoding = resp.apparent_encoding
    soup = BeautifulSoup(resp.text, "html.parser")

    # å¸¸è¦‹æ–°èç«™çš„å…§æ–‡å€å¡Š
    for selector in [
        "article",
        "div.article-content__editor",
        "div#story_body_content",
        "div.story",
        "section.article-body",
    ]:
        node = soup.select_one(selector)
        if node:
            text = node.get_text(separator=" ", strip=True)
            if len(text) > 100:
                return text

    # fallbackï¼šæŠ“æ‰€æœ‰ <p>
    return " ".join(
        [p.get_text(strip=True) for p in soup.find_all("p") if len(p.get_text()) > 10]
    )


# ==========================================================
# âœ¨ ç‰¹å¾µæŠ½å–
# ==========================================================
def extract_features(text: str) -> list:
    try:
        # æ–·è©
        if jieba:
            tokens = list(jieba.cut(text))
        else:
            tokens = text.split()

        word_count = len(tokens)

        # URL ç‰¹å¾µ
        url_match = re.search(r"https?://[^\s]+", text)
        has_url = 1 if url_match else 0
        domain_score = 0.5
        if has_url:
            domain = urlparse(url_match.group()).netloc
            domain_score = get_domain_score(domain)

        # èª‡å¼µç”¨èª & æƒ…ç·’å­—è©
        hyperbole_words = ["é©šäºº", "çˆ†æ–™", "éœ‡æ’¼", "çµ•å°", "çœŸç›¸", "æ›å…‰"]
        emotive_words = ["æ°£ç‚¸", "å“­äº†", "æ€’äº†", "æ…˜äº†", "è¶…æ‰¯"]

        hyperbole_score = sum(w in text for w in hyperbole_words) / len(
            hyperbole_words
        )
        emotive_score = sum(w in text for w in emotive_words) / len(emotive_words)

        # è‡ªè¨‚ç°¡æ˜“èªæ„å¼·åº¦
        semantic_strength = simple_embedding_score(text)

        # ç°¡å–®é•·åº¦æ¯”ä¾‹
        length_ratio = min(word_count / 200, 1.0)

        # éš¨æ©Ÿä¸€é»é»å™ªéŸ³ï¼Œè®“åˆ†æ•¸æ›´å¹³æ»‘ï¼ˆä¿æŒèˆ‡èˆŠæ¨¡å‹ç‰¹å¾µç¶­åº¦ä¸€è‡´ï¼‰
        random_noise = float(np.random.uniform(0.3, 0.9))

        return [
            round(domain_score, 2),
            1.0 if word_count > 50 else 0.5,
            round(hyperbole_score, 2),
            round(emotive_score, 2),
            has_url,
            semantic_strength,
            length_ratio,
            random_noise,
        ]
    except Exception as e:
        logging.error(f"âš ï¸ ç‰¹å¾µæ“·å–å¤±æ•—ï¼š{e}")
        return [0.5] * 8


# ==========================================================
# ğŸ”§ è¼”åŠ©å‡½å¼ç¾¤
# ==========================================================
def get_domain_score(domain: str) -> float:
    trusted = {
        "cna.com.tw": 5.0,
        "udn.com": 4.8,
        "ettoday.net": 4.2,
        "setn.com": 3.5,
        "businesstoday.com.tw": 3.8,
        "ltn.com.tw": 4.5,
    }
    return trusted.get(domain, 2.5)


def convert_score_to_label(score: float) -> str:
    if score >= 0.8:
        return "æ¥µé«˜"
    elif score >= 0.6:
        return "é«˜"
    elif score >= 0.4:
        return "ä¸­"
    elif score >= 0.2:
        return "ä½"
    else:
        return "æ¥µä½"


def generate_summary(text: str, score: float, level: str) -> str:
    desc = {
        "æ¥µé«˜": "å…§å®¹æ¸…æ™°ã€ä¾†æºç©©å®šï¼Œå¯ä¿¡åº¦æ¥µé«˜ã€‚",
        "é«˜": "èªæ°£ä¸­æ€§ã€å¼•ç”¨ä¾†æºæ˜ç¢ºï¼Œå¯ä¿¡åº¦åé«˜ã€‚",
        "ä¸­": "å¯ä¿¡åº¦ä¸­ç­‰ï¼Œå»ºè­°æ­é…ä¾†æºé€²ä¸€æ­¥æŸ¥è­‰ã€‚",
        "ä½": "å«èª‡å¼µæˆ–æƒ…ç·’åŒ–ç”¨èªï¼Œè«‹å°å¿ƒæ±‚è­‰ã€‚",
        "æ¥µä½": "ç–‘ä¼¼ä¸å¯¦æˆ–é‡£é­šè¨Šæ¯ï¼Œè«‹å‹¿è¼•ä¿¡æˆ–è½‰å‚³ã€‚",
        "æœªçŸ¥": "ç›®å‰ç„¡æ³•æ˜ç¢ºåˆ¤æ–·å¯ä¿¡åº¦ã€‚",
    }.get(level, "æœªçŸ¥å¯ä¿¡åº¦ã€‚")
    return f"æ¨¡å‹åˆ†æçµæœé¡¯ç¤ºï¼šå¯ä¿¡åº¦ç‚ºã€Œ{level}ã€ï¼ˆåˆ†æ•¸ {score:.2f}ï¼‰ã€‚{desc}"


def extract_keywords(text: str, top_k=5):
    if not jieba:
        return ""
    words = [w for w in jieba.cut(text) if len(w) > 1]
    freq = {}
    for w in words:
        freq[w] = freq.get(w, 0) + 1
    keywords = sorted(freq, key=freq.get, reverse=True)[:top_k]
    return ", ".join(keywords)


def guess_category(text: str):
    if any(k in text for k in ["é¸èˆ‰", "æ”¿åºœ", "æ”¿ç­–", "ç¸½çµ±"]):
        return "æ”¿æ²»"
    elif any(k in text for k in ["å½±åŠ‡", "å¶åƒ", "é›»å½±", "è—äºº", "æ¼”å”±æœƒ"]):
        return "å¨›æ¨‚"
    elif any(k in text for k in ["è©é¨™", "ç–«æƒ…", "å¥åº·", "é†«ç™‚", "çŠ¯ç½ª"]):
        return "ç¤¾æœƒ"
    else:
        return "å…¶ä»–"
