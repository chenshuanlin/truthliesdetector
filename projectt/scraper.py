import requests
from bs4 import BeautifulSoup
import pyperclip
import re
import urllib.parse
import time 
import json
import os
import argparse
import logging
from typing import List, Dict, Tuple, Optional
import numpy as np
import lightgbm as lgb
from pathlib import Path
# optional imports for external services
try:
    from duckduckgo_search import DDGS
except Exception:
    DDGS = None
try:
    from serpapi import GoogleSearch
except Exception:
    GoogleSearch = None
try:
    import google.generativeai as genai
except Exception:
    genai = None
    types = None

# ==============================================================================
# I. ç³»çµ±é…ç½®èˆ‡æœå‹™é‡‘é‘°
# ==============================================================================

DOMAIN_CREDIBILITY_MAP = {
    'cna.com.tw': 5.0, 'udn.com': 4.5, 'setn.com': 3.0, 'www.facebook.com': 2.5,
    'ptt.cc': 2.0, 'bogus-news.xyz': 1.0
}
EMOTIONAL_INDICATORS = [
    'é©šäºº', 'çµ•å°', 'éœ‡é©š', 'é›¢è­œ', 'ä¸å¯æ€è­°', 'å¤§çˆ†ç™¼', 'å°å¿ƒ', 'æ…˜äº†', 
    'æ€’å¼', 'å´©æ½°', 'ç¨å®¶', 'æ€¥è½‰ç›´ä¸‹', 'é¦¬ä¸Šçœ‹', 'ç˜‹å‚³', 'ç§˜å¯†'
]

# ğŸ’¡ SerpApi é‡‘é‘° (æ‚¨çš„ Google æœå°‹å‚™æ´é‡‘é‘°)
SERPAPI_API_KEY = "d74cf3f39503404c0426005f0c23cc59246f60084b198b8dcbee955b04448452"

# âš ï¸ Gemini API é‡‘é‘° (ç”¨æ–¼ LLM æ·±åº¦åˆ†æï¼Œè«‹å‹™å¿…æ›¿æ›ï¼)
GEMINI_API_KEY = "AIzaSyBZoPr5y8AM3c9VcM5ahIAqfw0ODtRAtQk"

# ==============================================================================
# II. å…§å®¹æ“·å–èˆ‡é è™•ç†æ¨¡çµ„ (Extraction & Preprocessing) (ç•¥ï¼Œèˆ‡ä¸Šä¸€ç‰ˆç›¸åŒ)
# ==============================================================================

def preprocess_document_text(text: str) -> str:
    """æ¸…ç†å¸¸è¦‹çš„ç¶²é å™ªéŸ³ã€å»£å‘Šå’Œå†—é¤˜ç©ºé–“ã€‚"""
    text = re.sub(r'\(C\) ç‰ˆæ¬Šæ‰€æœ‰|All rights reserved|åˆ†äº«çµ¦å¥½å‹|é»æ“Šä¸‹è¼‰|ç¹¼çºŒé–±è®€|ç›¸é—œæ–°è.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\n{2,}', '\n', text) 
    text = re.sub(r'\s{2,}', ' ', text).strip()
    return text

def fetch_and_clean_url(url: str) -> Tuple[str, str, str]:
    """å¾ URL æŠ“å–æ¨™é¡Œã€ç¶²åŸŸå’Œæ·¨åŒ–å¾Œçš„ä¸»æ–‡æœ¬ã€‚"""
    print(f"-> æ­£åœ¨æ“·å–ç¶²å€å…§å®¹: {url}")
    domain = urllib.parse.urlparse(url).netloc
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    max_attempts = 3
    for attempt in range(1, max_attempts + 1):
        try:
            time.sleep(1.0 * attempt)  # backoff
            response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
            response.raise_for_status()
            response.encoding = response.apparent_encoding

            soup = BeautifulSoup(response.text, 'html.parser')
            title = soup.title.string.strip() if soup.title and soup.title.string else "æœªåµæ¸¬åˆ°æ¨™é¡Œ"

            main_text = ""
            # ä½¿ç”¨ CSS selector å˜—è©¦æ“·å–æ–‡ç« ä¸»é«”ï¼ˆæ¯”ç›´æ¥ find æ›´å¯é ï¼‰
            content_selectors = ['article', 'div[itemprop="articleBody"]', 'div.article-content', 'div.entry-content', 'div#main-content']

            for selector in content_selectors:
                article_body = soup.select_one(selector)
                if article_body and len(article_body.get_text(strip=True)) > 200:
                    main_text = article_body.get_text(separator='\n', strip=True)
                    break

            if not main_text and soup.body:
                main_text = soup.body.get_text(separator='\n', strip=True)

            main_text = preprocess_document_text(main_text)
            return title, domain, main_text

        except requests.exceptions.RequestException as e:
            logging.debug(f"fetch attempt {attempt} failed for {url}: {e}")
            if attempt == max_attempts:
                error_msg = f"ç¶²è·¯é€£ç·šæˆ–è«‹æ±‚éŒ¯èª¤ (ERR: {e.__class__.__name__})"
                return "æå–å¤±æ•—", domain, error_msg
            continue
        except Exception as e:
            logging.debug(f"processing error for {url}: {e}")
            error_msg = f"è³‡æ–™è™•ç†ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ (ERR: {e.__class__.__name__})"
            return "æå–å¤±æ•—", domain, error_msg

# ==============================================================================
# III. æœå°‹èˆ‡å‚™æ´æ¨¡çµ„ (Search & Fallback) (ç•¥ï¼Œèˆ‡ä¸Šä¸€ç‰ˆç›¸åŒ)
# ==============================================================================

def perform_ddgs_search(query: str, max_results: int = 20) -> List[Dict[str, str]]:
    """ä½¿ç”¨ DuckDuckGo (DDGS) é€²è¡Œé—œéµå­—æœå°‹ã€‚"""
    print(f"-> æ­£åœ¨åŸ·è¡Œ DuckDuckGo é—œéµå­—æœå°‹: {query} (æœ€å¤š {max_results} ç­†)")
    results = []
    
    try:
        with DDGS() as ddgs:
            # ğŸ DDGS éŒ¯èª¤ä¿®å¾©ï¼šä½¿ç”¨é—œéµå­—åƒæ•¸ q=query
            ddgs_results = ddgs.text(q=query, region='tw-zh', max_results=max_results)
            
            for r in ddgs_results:
                results.append({'title': r['title'], 'link': r['href'], 'snippet': r.get('body', 'ç„¡æ‘˜è¦')})
            time.sleep(3) 
    except Exception as e:
        print(f" Â > DDGS æœå°‹å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–æœå°‹é »ç‡: {e}")
            
    return results

def perform_serpapi_fallback(query: str, max_results: int = 10) -> List[Dict[str, str]]:
    """ä½¿ç”¨ SerpApi (Google) é€²è¡Œå‚™æ´æœå°‹ã€‚"""
    print(f"-> æ­£åœ¨åŸ·è¡Œ SerpApi å‚™ç”¨æœå°‹ (Google): {query} (æœ€å¤š {max_results} ç­†)")
    
    if not SERPAPI_API_KEY or SERPAPI_API_KEY == "YOUR_API_KEY_HERE":
        print(" Â > âŒ SerpApi é‡‘é‘°æœªè¨­ç½®ï¼å‚™æ´æœå°‹ä¸­æ­¢ã€‚")
        return []

    params = {
        "engine": "google", "q": query, "api_key": SERPAPI_API_KEY,
        "gl": "tw", "hl": "zh-tw", "num": max_results
    }
    
    results = []
    try:
        search = GoogleSearch(params)
        data = search.get_dict()

        if "organic_results" in data:
            for item in data["organic_results"]:
                results.append({
                    'title': item.get('title', 'ç„¡æ¨™é¡Œ'), 
                    'link': item.get('link', ''),
                    'snippet': item.get('snippet', 'ç„¡æ‘˜è¦')
                })
        
        return results

    except Exception as e:
        print(f" Â > âŒ SerpApi æœå°‹å¤±æ•—ã€‚è«‹æª¢æŸ¥é‡‘é‘°æˆ–é¡åº¦ã€‚éŒ¯èª¤: {e}") 
        return []

# ==============================================================================
# IV. ç‰¹å¾µå·¥ç¨‹æ¨¡çµ„ (Feature Engineering) (ç•¥ï¼Œèˆ‡ä¸Šä¸€ç‰ˆç›¸åŒ)
# ==============================================================================

def get_domain_credibility(domain: str) -> float:
    """æ ¹æ“šé è¨­å°æ‡‰è¡¨ç²å–ç¶²åŸŸå¯ä¿¡åº¦åˆ†æ•¸ã€‚"""
    normalized_domain = domain.replace('www.', '')
    return DOMAIN_CREDIBILITY_MAP.get(normalized_domain, 3.0)

def calculate_article_features(url: str, title: str, content: str, domain: str) -> Dict[str, float]:
    """è¨ˆç®—æ–‡ç« å…§å®¹çš„çµæ§‹åŒ–ç‰¹å¾µï¼Œä¾› LLM åˆ¤åˆ¥åƒè€ƒã€‚"""
    features = {}
    
    if "æå–å¤±æ•—" in title or not content:
        return {'score_source': 1.0, 'emotion_ratio': 0.0, 'text_length': 0.0, 'final_crawler_score': 1.0, 'punctuation_density': 0.0}

    features['score_source'] = get_domain_credibility(domain) 
    text_len = len(content)
    features['text_length'] = float(text_len)

    emotion_count = sum(content.count(kw) for kw in EMOTIONAL_INDICATORS)
    features['emotion_ratio'] = (emotion_count / (text_len / 100)) if text_len > 100 else float(emotion_count) 
        
    exclamation_count = content.count('!') + content.count('ï¼')
    question_count = content.count('?') + content.count('ï¼Ÿ')
    features['punctuation_density'] = (exclamation_count + question_count) / (text_len / 100) if text_len > 100 else float(exclamation_count + question_count) 

    crawler_score = features['score_source']
    crawler_score -= (features['emotion_ratio'] * 0.5) 
    crawler_score -= (features['punctuation_density'] * 0.2) 
    
    features['final_crawler_score'] = max(1.0, min(5.0, crawler_score))
    
    return features

# ==============================================================================
# V. AI åˆ¤åˆ¥æœå‹™å®¢æˆ¶ç«¯ (LLM Client Interface) (ç•¥ï¼Œèˆ‡ä¸Šä¸€ç‰ˆç›¸åŒ)
# ==============================================================================

class AnalysisOutput:
    """LLM æœå‹™çš„çµæ§‹åŒ–è¼¸å‡ºç‰©ä»¶ã€‚"""
    def __init__(self, confidence_score: float, credibility_level: str, summary: str):
        self.confidence_score = confidence_score
        self.credibility_level = credibility_level
        self.summary = summary

class CredibilityAnalyzerClient:
    """ç”¨æ–¼èˆ‡ Gemini æœå‹™äº’å‹•ï¼Œå¯¦ç¾æ·±åº¦å¯ä¿¡åº¦åˆ¤åˆ¥çš„å®¢æˆ¶ç«¯ä»‹é¢ã€‚"""
    def __init__(self, api_key: str):
        if api_key == "YOUR_GEMINI_API_KEY_HERE":
            self.client = None
            print("âŒ LLM å®¢æˆ¶ç«¯ï¼šGemini API é‡‘é‘°æœªè¨­ç½®ã€‚å°‡é€²å…¥ã€æ¨¡æ“¬æ¨¡å¼ã€‘ã€‚")
        else:
            try:
                # ä½¿ç”¨æ–°ç‰ˆ Gemini API åˆå§‹åŒ–æ–¹å¼
                if genai is not None:
                    genai.configure(api_key=api_key)
                    self.client = genai
                    print("[OK] LLM å®¢æˆ¶ç«¯ï¼šGemini API åˆå§‹åŒ–æˆåŠŸã€‚")
                else:
                    self.client = None
                    print("âŒ LLM å®¢æˆ¶ç«¯ï¼šgoogle-generativeai æ¨¡çµ„æœªå®‰è£ã€‚å°‡é€²å…¥ã€æ¨¡æ“¬æ¨¡å¼ã€‘ã€‚")
            except Exception as e:
                 self.client = None
                 print(f"âŒ LLM å®¢æˆ¶ç«¯ï¼šGemini API åˆå§‹åŒ–å¤±æ•— ({e})ã€‚å°‡é€²å…¥ã€æ¨¡æ“¬æ¨¡å¼ã€‘ã€‚")

    def perform_llm_analysis(self, title: str, content: str, features: Dict[str, float]) -> AnalysisOutput:
        """èª¿ç”¨ LLM æ¨¡å‹æ ¹æ“šæ–‡ç« å…§å®¹å’Œç‰¹å¾µé€²è¡Œå°ˆæ¥­åˆ†æã€‚"""
        
        if self.client is None:
            time.sleep(1)
            sim_score = min(1.0, features['final_crawler_score'] / 5.0 + 0.1) 
            sim_level = "ä¸­åº¦å¯ä¿¡/å¯ç–‘ (æ¨¡æ“¬)" if 0.5 < sim_score <= 0.8 else "ä½åº¦å¯ä¿¡ (æ¨¡æ“¬)"
            sim_summary = (
                f"ã€æ¨¡æ“¬çµæœã€‘: è©²æ–‡ç« ç¶“ç”±åŸºç¤çˆ¬èŸ²ç‰¹å¾µåˆ†æï¼Œåˆ†æ•¸ç‚º {sim_score:.3f}ã€‚"
                f"ï¼ˆè«‹æä¾›æœ‰æ•ˆçš„ Gemini é‡‘é‘°ä»¥å•Ÿç”¨å°ˆæ¥­ LLM è©•è«–ï¼‰"
            )
            return AnalysisOutput(sim_score, sim_level, sim_summary)

        try:
            prompt = f"""
            ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è³‡è¨Šå¯ä¿¡åº¦åˆ†æå¸«ã€‚è«‹æ ¹æ“šæä¾›çš„æ–‡ç« å…§å®¹å’Œçˆ¬èŸ²è¨ˆç®—å‡ºçš„ç‰¹å¾µæŒ‡æ¨™ï¼Œåˆ¤æ–·é€™ç¯‡æ–‡ç« çš„å¯ä¿¡åº¦ç­‰ç´š (Credibility Level) ä¸¦çµ¦å‡ºåˆ†æ•¸ (Score)ã€‚
            æ–‡ç« æ¨™é¡Œ: {title}
            æ–‡ç« å…§å®¹æ‘˜è¦: {content[:1500]}...
            çˆ¬èŸ²è¨ˆç®—çš„åŸºç¤ç¸½åˆ† (5.0æ»¿åˆ†): {features['final_crawler_score']:.2f}
            ä¾†æºç¶²åŸŸå¯ä¿¡åº¦: {features['score_source']:.2f}
            æ–‡ç« æƒ…ç·’åŒ–è©å½™æ¯”ä¾‹ (æ¯ç™¾å­—): {features['emotion_ratio']:.2f}

            è«‹åš´æ ¼ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•é¡å¤–æ–‡å­—ã€‚JSON æ ¼å¼å¿…é ˆåŒ…å«ä¸‰å€‹éµï¼š
            1. credibility_level: æœ€çµ‚å¯ä¿¡åº¦ç­‰ç´šï¼ˆä¾‹å¦‚ï¼šã€Œé«˜åº¦å¯ä¿¡ã€ã€ã€Œä¸­åº¦å¯ç–‘ã€ã€ã€Œæ¥µåº¦å¯ç–‘ã€ï¼‰ã€‚
            2. confidence_score: æœ€çµ‚å¯ä¿¡åº¦åˆ†æ•¸ï¼ˆ0.0åˆ°1.0ä¹‹é–“ï¼Œè¶Šé«˜è¶Šå¯ä¿¡ï¼‰ã€‚
            3. summary: åŸºæ–¼å…§å®¹å’Œç‰¹å¾µçš„å°ˆæ¥­åˆ†æç¸½çµï¼ˆç´„100å­—ï¼‰ã€‚
            """
            
            # ä½¿ç”¨æ–°ç‰ˆ Gemini API
            model = self.client.GenerativeModel(
                'gemini-2.0-flash',
                generation_config={
                    "response_mime_type": "application/json",
                    "response_schema": {
                        "type": "object",
                        "properties": {
                            "credibility_level": {"type": "string"},
                            "confidence_score": {"type": "number"},
                            "summary": {"type": "string"},
                        },
                        "required": ["credibility_level", "confidence_score", "summary"],
                    },
                }
            )
            
            response = model.generate_content(prompt)
            
            data = json.loads(response.text)
            
            return AnalysisOutput(
                confidence_score=data.get('confidence_score', 0.5),
                credibility_level=data.get('credibility_level', 'åˆ†æå¤±æ•—'),
                summary=f"ã€LLM å°ˆæ¥­åˆ†æã€‘: {data.get('summary', 'ç„¡è©³ç´°è©•è«–')}"
            )

        except Exception as e:
            print(f"âŒ LLM å‘¼å«æˆ–è§£æå¤±æ•—ã€‚éŒ¯èª¤é¡å‹: {e.__class__.__name__}")
            sim_score = min(1.0, features['final_crawler_score'] / 5.0 + 0.1) 
            sim_level = "éŒ¯èª¤å›é€€ï¼šä¸­åº¦å¯ä¿¡/å¯ç–‘"
            sim_summary = "ã€LLM éŒ¯èª¤å›é€€ã€‘: LLM æœå‹™å‘¼å«å¤±æ•—ï¼Œåˆ†æ•¸ç‚ºçˆ¬èŸ²åŸºç¤åˆ†æ•¸çš„ç·šæ€§æ˜ å°„ã€‚"
            return AnalysisOutput(sim_score, sim_level, sim_summary)

    def annotate_features(self, title: str, content: str, url: str = "") -> Tuple[Dict[str, float], str]:
        """ä½¿ç”¨ Geminiï¼ˆæˆ–æ¨¡æ“¬ï¼‰è¼¸å‡ºè©²æ–‡ç« çš„ç‰¹å¾µæ¨™è¨»èˆ‡ 30 å­—ä»¥å…§åˆ¤æ–·ã€‚
        å›å‚³ (features_dict, short_judgement)ã€‚features_dict çš„ keys èˆ‡æ‚¨æä¾›çš„ schema ç›¸ç¬¦ï¼Œå€¼åœ¨ 0.0-1.0ã€‚
        """
        # If no real client, produce a heuristic simulation
        if self.client is None:
            # Heuristics based on domain and content
            domain = urllib.parse.urlparse(url).netloc if url else ""
            dscore = get_domain_credibility(domain) / 5.0

            # title-body overlap as simple ratio
            title_tokens = set(re.findall(r"\w+", title.lower()))
            body_tokens = set(re.findall(r"\w+", content.lower()))
            if title_tokens and body_tokens:
                overlap = len(title_tokens & body_tokens) / max(1, len(title_tokens))
            else:
                overlap = 0.0

            # emotion density from previous feature calculation
            emotion_count = sum(content.count(kw) for kw in EMOTIONAL_INDICATORS)
            text_len = max(1, len(content))
            emotive_density = min(1.0, (emotion_count / (text_len / 100)))

            # evidence quality approx: presence of numbers/doi/https links
            evidence = 0.0
            if re.search(r"\b\d{4}\b", content):
                evidence += 0.3
            if re.search(r"doi:\/\/|doi\.org|pubmed|arxiv", content, flags=re.I):
                evidence += 0.5
            if re.search(r"https?:\/\/(?:[\w.-]+)\/(?:\S*\d)", content):
                evidence += 0.2
            evidence = min(1.0, evidence)

            # ad intensity: presence of buy/subscribe/discount keywords
            ad = 0.0
            if re.search(r"buy now|subscribe|discount|promo|è³¼è²·|è¯ç›Ÿ|å»£å‘Š|è´ŠåŠ©", content, flags=re.I):
                ad = 0.7

            features = {
                "source_entity_score": dscore,  # rough
                "domain_score": dscore,
                "title_body_consistency": min(1.0, overlap * 1.2),
                "evidence_quality": evidence,
                "ad_promo_intensity": ad,
                "hyperbole_score": min(1.0, emotive_density * 0.8),
                "emotive_clickbait_density": emotive_density,
                "title_body_embedding_cosine": min(1.0, overlap),
            }

            # short judgement (<=30 chars)
            short = None
            if features["domain_score"] >= 0.8 and features["evidence_quality"] >= 0.6:
                short = "é«˜åº¦å¯ä¿¡"
            elif features["emotive_clickbait_density"] > 0.6 or features["ad_promo_intensity"]>0.7:
                short = "å¯ç–‘ / å»£å‘Šå°å‘"
            elif features["title_body_consistency"] < 0.4:
                short = "æ¨™é¡Œèˆ‡å…§æ–‡ä¸ç¬¦"
            else:
                short = "ä¸­åº¦å¯ä¿¡"

            return features, short

        # Real Gemini call: ask for JSON with numeric fields + short judgement
        try:
            # build prompt safely (avoid f-string to prevent parsing issues)
            prompt = (
                "è«‹ä»¥ JSON æ ¼å¼å›å‚³ä¸‹åˆ—æ¬„ä½ (æ•¸å€¼ 0.0 åˆ° 1.0)ï¼š\n\n"
                + "title: " + (title or "") + "\n\n"
                + "content: " + (content or "")[:2000].replace('\n', ' ') + "...\n\n"
                + "æ¬„ä½: source_entity_score, domain_score, title_body_consistency, evidence_quality, ad_promo_intensity, hyperbole_score, emotive_clickbait_density, title_body_embedding_cosine\n"
                + "åŒæ™‚å›å‚³ä¸€å€‹ short_judgement (ä¸è¶…é 30 å€‹å­—çš„ä¸­æ–‡åˆ¤æ–·)ã€‚\n"
                + "JSON å¿…é ˆåªæœ‰ä¸€å€‹ç‰©ä»¶ï¼Œç¯„ä¾‹å¦‚ï¼š{\"source_entity_score\":0.8, \"domain_score\":0.9, \"title_body_consistency\":0.8, \"evidence_quality\":0.7, \"ad_promo_intensity\":0.1, \"hyperbole_score\":0.2, \"emotive_clickbait_density\":0.1, \"title_body_embedding_cosine\":0.85, \"short_judgement\":\"é«˜åº¦å¯ä¿¡\"}\n"
            )

            # ä½¿ç”¨æ–°ç‰ˆ Gemini API
            model = self.client.GenerativeModel(
                'gemini-2.0-flash',
                generation_config={
                    "response_mime_type": "application/json",
                    "response_schema": {
                        "type": "object",
                        "properties": {
                            "source_entity_score": {"type": "number"},
                            "domain_score": {"type": "number"},
                            "title_body_consistency": {"type": "number"},
                            "evidence_quality": {"type": "number"},
                            "ad_promo_intensity": {"type": "number"},
                            "hyperbole_score": {"type": "number"},
                            "emotive_clickbait_density": {"type": "number"},
                            "title_body_embedding_cosine": {"type": "number"},
                            "short_judgement": {"type": "string"},
                        },
                        "required": ["source_entity_score","domain_score","title_body_consistency","evidence_quality","ad_promo_intensity","hyperbole_score","emotive_clickbait_density","title_body_embedding_cosine","short_judgement"]
                    }
                }
            )

            response = model.generate_content(prompt)
            data = json.loads(response.text)
            # normalize numbers into 0.0-1.0 range
            features = {k: float(data.get(k, 0.0)) for k in [
                "source_entity_score","domain_score","title_body_consistency","evidence_quality","ad_promo_intensity","hyperbole_score","emotive_clickbait_density","title_body_embedding_cosine"
            ]}
            short = data.get("short_judgement", "")[:30]
            return features, short
        except Exception as e:
            logging.debug(f"Gemini annotate failed: {e}")
            # fallback to heuristic simulation
            features = {
                "source_entity_score": 0.5,
                "domain_score": 0.5,
                "title_body_consistency": 0.6,
                "evidence_quality": 0.5,
                "ad_promo_intensity": 0.3,
                "hyperbole_score": 0.4,
                "emotive_clickbait_density": 0.3,
                "title_body_embedding_cosine": 0.7
            }
            short = "æ¨¡æ“¬åˆ¤æ–·"
            return features, short

# ==============================================================================
# VI. å ±å‘Šç”Ÿæˆæ¨¡çµ„ (Report Generation Module)
# ==============================================================================

def generate_final_report(
    user_input: str, 
    mode: str, 
    llm_report: Optional[AnalysisOutput] = None, 
    features: Optional[Dict[str, float]] = None, 
    all_analyses: Optional[List[Dict]] = None, 
    content: str = ""
) -> str:
    """æ ¹æ“šåˆ†ææ¨¡å¼ç”Ÿæˆçµæ§‹åŒ–å ±å‘Šã€‚"""
    
    report = ["\n" + "="*80]
    report.append("Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ã€è³‡è¨Šå¯ä¿¡åº¦åˆ†æå ±å‘Šã€‘")
    report.append(f"ç”Ÿæˆæ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}")
    report.append("="*80)

    if mode == "URL":
        # --- å–®ä¸€ç¶²å€å ±å‘Š ---
        report.append(f"ã€åˆ†æé¡å‹ã€‘: å–®ä¸€ç¶²å€æ·±åº¦åˆ†æ")
        report.append(f"ã€ç›®æ¨™ç¶²å€ã€‘: {user_input}")
        report.append(f"ã€ç¶²é æ¨™é¡Œã€‘: {content[:100].splitlines()[0]}...")
        report.append("-" * 30)
        
        if llm_report and features:
            report.append("--- ğŸ¤– LLM æ·±åº¦åˆ¤åˆ¥çµæœ ---")
            report.append(f"æœ€çµ‚å¯ä¿¡åº¦ç­‰ç´š: {llm_report.credibility_level}")
            report.append(f"LLM è©•ä¼°åˆ†æ•¸ (0.0-1.0): {llm_report.confidence_score:.3f}")
            report.append(f"åˆ†æç¸½çµ:\n{llm_report.summary}")
            report.append("-" * 30)
            
            report.append("--- ğŸ“Š çˆ¬èŸ²ç‰¹å¾µæŒ‡æ¨™ ---")
            report.append(f"  - ç¶²åŸŸåŸºç¤åˆ†æ•¸ (5.0): {features['score_source']:.2f}")
            report.append(f"  - æ–‡ç« ç¸½å­—æ•¸: {features['text_length']:.0f}")
            report.append(f"  - æƒ…ç·’åŒ–æŒ‡æ¨™ (æ¯ç™¾å­—): {features['emotion_ratio']:.2f}")
            report.append(f"  - çˆ¬èŸ²åŸºç¤ç¸½åˆ† (5.0): {features['final_crawler_score']:.2f}")
        
        report.append("="*80)
        report.append(f"ã€æ–‡ç« å…§å®¹ç‰‡æ®µã€‘:\n{content[:1500]}...")

    elif mode == "KEYWORD" and all_analyses:
        # --- é—œéµå­—æ‰¹é‡åˆ†æå ±å‘Š ---
        report.append(f"ã€åˆ†æé¡å‹ã€‘: é—œéµå­—æ‰¹é‡åˆ†æ")
        report.append(f"ã€æœå°‹é—œéµå­—ã€‘: {user_input}")
        report.append(f"ã€æœ‰æ•ˆåˆ†ææ–‡ç« æ•¸ã€‘: {len(all_analyses)}")
        report.append("-" * 80)
        
        report.append("  ç´¢å¼• | LLM ç­‰ç´šåŠåˆ†æ•¸ | ç¶²åŸŸ | æ–‡ç« æ¨™é¡Œ")
        report.append("-" * 80)
        
        for analysis in all_analyses:
            # æ ¼å¼åŒ–è¼¸å‡ºï¼Œè®“åˆ†æ•¸å°é½Š
            score_str = f"{analysis['ai_score']:.3f}"
            title_summary = analysis['title'][:40].ljust(40)
            line = (
                f" {analysis['index']:<4} | {analysis['ai_level'].ljust(10)} ({score_str}) | "
                f"{analysis['domain'].ljust(15)} | {title_summary}..."
            )
            report.append(line)
        
        report.append("-" * 80)
        report.append("\nã€å»ºè­°ã€‘: æ‡‰ç‰¹åˆ¥é—œæ³¨è©•åˆ†ä½æ–¼ 0.5 çš„é€£çµï¼Œä¸¦æŸ¥çœ‹å…¶åŸå§‹å…§å®¹ã€‚")
        
    else:
        # æœå°‹ç„¡çµæœå ±å‘Š
        report.append(f"ã€åˆ†æé¡å‹ã€‘: é—œéµå­—æœå°‹")
        report.append(f"ã€æœå°‹é—œéµå­—ã€‘: {user_input}")
        report.append("Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â æœªæ‰¾åˆ°ä»»ä½•ç›¸é—œä¸”å¯åˆ†æçš„é€£çµã€‚")
        report.append("-" * 80)

    report.append("\n\n")
    return "\n".join(report)

# ==============================================================================
# VII. ä¸»æ‡‰ç”¨ç¨‹å¼é‚è¼¯ (Main Application Logic)
# ==============================================================================

def is_valid_url(text: str) -> bool:
    """æª¢æŸ¥è¼¸å…¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ URL æ ¼å¼ã€‚"""
    return text.startswith(('http://', 'https://'))

def run_analysis_system():
    # å˜—è©¦åœ¨å°ˆæ¡ˆä¸­è‡ªå‹•æ‰¾åˆ°æ¨¡å‹æª”ï¼Œå°‡å…¶è¨­ç‚º --model çš„é è¨­å€¼
    discovered_model = None
    try:
        candidates = list(Path('.').rglob('*auth_level*.txt'))
        if candidates:
            # ä½¿ç”¨ç›¸å°è·¯å¾‘ï¼ˆå¾å°ˆæ¡ˆæ ¹ï¼‰ä½œç‚ºé è¨­
            discovered_model = str(candidates[0].as_posix())
    except Exception:
        discovered_model = None

    parser = argparse.ArgumentParser(description='è³‡è¨Šå¯ä¿¡åº¦è¼”åŠ©ç³»çµ±')
    parser.add_argument('--max-results', type=int, default=20, help='é—œéµå­—æœå°‹çš„æœ€å¤§çµæœæ•¸é‡')
    parser.add_argument('--min-length', type=int, default=100, help='æœ€å°‘æ–‡ç« é•·åº¦ (å­—å…ƒ) æ‰é€²è¡Œåˆ†æ')
    parser.add_argument('--save', action='store_true', help='åŒæ™‚å°‡å®Œæ•´å ±å‘Šå­˜æˆæª”æ¡ˆ')
    parser.add_argument('--verbose', action='store_true', help='å•Ÿç”¨è©³ç´°æ—¥èªŒ')
    parser.add_argument('--model', default=(discovered_model or 'model_auth_level/auth_level_lgbm.txt'), help='LightGBM æ¨¡å‹æª”æ¡ˆè·¯å¾‘ (å¯é¸)')
    parser.add_argument('--query', default=None, help='éäº’å‹•å¼æŒ‡å®šæœå°‹é—œéµå­—ï¼ˆé¿å…ä½¿ç”¨ç®¡é“ï¼‰')
    args = parser.parse_args()

    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)

    print("--- è³‡è¨Šå¯ä¿¡åº¦è¼”åŠ©ç³»çµ±ï¼šå…§å®¹æ“·å–èˆ‡ LLM æ·±åº¦åˆ†æ---")

    analyzer = CredibilityAnalyzerClient(api_key=GEMINI_API_KEY)
    # load LightGBM model if provided
    booster = None
    model_path = Path(args.model)
    # If the configured model path doesn't exist, attempt to auto-discover a model in the repo
    if not model_path.exists():
        print(f"âš ï¸ æŒ‡å®šæ¨¡å‹ä¸å­˜åœ¨: {model_path}ï¼Œå˜—è©¦åœ¨å°ˆæ¡ˆä¸­è‡ªå‹•æœå°‹ç›¸ä¼¼æ¨¡å‹æª”...")
        candidates = list(Path('.').rglob('*auth_level*.txt'))
        if candidates:
            # prefer the first reasonable candidate
            model_path = candidates[0]
            print(f"ğŸ” è‡ªå‹•æ‰¾åˆ°æ¨¡å‹æª”: {model_path}ï¼Œå°‡ä½¿ç”¨æ­¤æ¨¡å‹é€²è¡Œé æ¸¬ã€‚")
        else:
            print(f"âŒ æœªåœ¨å°ˆæ¡ˆä¸­æ‰¾åˆ°ä»»ä½• '*auth_level*.txt' æ¨¡å‹æª”ï¼Œå°‡ç•¥éæ¨¡å‹é æ¸¬ã€‚")

    if model_path.exists():
        try:
            booster = lgb.Booster(model_file=str(model_path))
            print(f"[OK] å·²è¼‰å…¥æ¨¡å‹: {model_path}")
        except Exception as e:
            print(f"[ERROR] è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
            booster = None

    if args.query:
        user_input = args.query.strip()
        print(f"ä½¿ç”¨ --query æä¾›æœå°‹å­—ä¸²: {user_input}")
    else:
        user_input = input("è«‹è¼¸å…¥ã€ç›®æ¨™ç¶²å€ (e.g., https://...)ã€‘æˆ–ã€é—œéµå­—ã€‘ï¼š").strip()

    output_content = ""
    results: List[Dict[str, str]] = []
    all_analyses: List[Dict] = []
    skipped: List[Tuple[str, str]] = []  # list of (url, reason)
    llm_report: Optional[AnalysisOutput] = None

    if is_valid_url(user_input):
        # --- æ¨¡å¼ A: å–®ä¸€ç¶²å€æ“·å– ---
        title, domain, content = fetch_and_clean_url(user_input)

        if "æå–å¤±æ•—" in title or not content or len(content) < args.min_length:
            print("\nâŒ ç¶²å€å…§å®¹æ“·å–å¤±æ•—æˆ–å…§å®¹ä¸è¶³ã€‚")
            output_items = []
        else:
            output_items = [{
                'title': title,
                'url': user_input,
                'domain': domain,
                'content': content,
            }]

        # annotate and model-predict for single URL
        enriched = []
        for it in output_items:
            feats_ann, short = analyzer.annotate_features(it['title'], it['content'], it['url'])
            it['ann_features'] = feats_ann
            it['short_judgement'] = short
            if booster is not None:
                # convert to numpy vector in same order as model expects
                feat_order = [
                    "source_entity_score","domain_score","title_body_consistency","evidence_quality","ad_promo_intensity","hyperbole_score","emotive_clickbait_density","title_body_embedding_cosine"
                ]
                x = np.array([feats_ann.get(k, 0.0) for k in feat_order], dtype=float).reshape(1, -1)
                try:
                    proba = booster.predict(x, num_iteration=booster.best_iteration or None)
                    pred = int(np.argmax(proba, axis=1)[0])
                    it['model_score'] = int(pred)
                    it['model_proba'] = proba[0].tolist()
                except Exception as e:
                    it['model_score'] = None
                    it['model_proba'] = []
            enriched.append(it)

        output_content = json.dumps({'mode': 'URL', 'items': enriched}, ensure_ascii=False, indent=2)

        output_content = json.dumps({'mode': 'URL', 'items': output_items}, ensure_ascii=False, indent=2)
        print("\n[OK] å–®ä¸€ç¶²å€æ“·å–å®Œæˆï¼Œè¼¸å‡º JSONï¼š\n")
        print(output_content)
        # å­˜æª”æ”¹ç‚ºåŸ·è¡ŒçµæŸæ™‚çµ±ä¸€å¯«å…¥ï¼ˆè¦‹ç¨‹å¼å°¾ç«¯ï¼‰

    else:
        # --- æ¨¡å¼ B: é—œéµå­—æ‰¹é‡åˆ†æ ---
        results = perform_ddgs_search(user_input, max_results=args.max_results)

        if not results:
            results = perform_serpapi_fallback(user_input, max_results=min(10, args.max_results))

        if results:
            print("\n-> é–‹å§‹æ‰¹é‡æ“·å–æœå°‹çµæœçš„æ¨™é¡Œ/ç¶²å€/å…§å®¹...")
            items = []
            for i, item in enumerate(results, 1):
                url = item.get('link')
                if not url:
                    skipped.append((str(item), 'no link'))
                    continue

                title, domain, content = fetch_and_clean_url(url)

                if "æå–å¤±æ•—" in title:
                    skipped.append((url, 'fetch failed'))
                    continue

                if not content or len(content) < args.min_length:
                    skipped.append((url, f'content too short ({len(content) if content else 0})'))
                    continue

                # åŠ å…¥çˆ¬å–æ™‚é–“æˆ³è¨˜
                from datetime import datetime
                crawled_at = datetime.now().isoformat()

                entry = {
                    'index': i,
                    'title': title,
                    'url': url,
                    'domain': domain,
                    'content': content,
                    'crawled_at': crawled_at,  # æ–°å¢ï¼šè¨˜éŒ„çˆ¬å–æ™‚é–“
                }

                # annotate with Gemini or heuristic and predict with model
                feats_ann, short = analyzer.annotate_features(title, content, url)
                entry['ann_features'] = feats_ann
                entry['short_judgement'] = short
                if booster is not None:
                    feat_order = [
                        "source_entity_score","domain_score","title_body_consistency","evidence_quality","ad_promo_intensity","hyperbole_score","emotive_clickbait_density","title_body_embedding_cosine"
                    ]
                    x = np.array([feats_ann.get(k, 0.0) for k in feat_order], dtype=float).reshape(1, -1)
                    try:
                        proba = booster.predict(x, num_iteration=booster.best_iteration or None)
                        pred = int(np.argmax(proba, axis=1)[0])
                        entry['model_score'] = int(pred)
                        entry['model_proba'] = proba[0].tolist()
                    except Exception as e:
                        entry['model_score'] = None
                        entry['model_proba'] = []

                items.append(entry)

                print(f"  > [OK] [{i}] {domain} æ“·å–å®Œæˆã€‚")

            output_content = json.dumps({'mode': 'KEYWORD', 'items': items, 'skipped_count': len(skipped)}, ensure_ascii=False, indent=2)
            print(f"\n[OK] æ‰¹é‡æ“·å–å®Œæˆï¼å…±æ“·å–: {len(items)} ç¯‡ï¼›è¢«è·³é: {len(skipped)} ç¯‡ã€‚")
            if skipped:
                print("\n-- è¢«è·³éçš„é€£çµ (ç¯„ä¾‹æœ€å¤š 10 ç­†) --")
                for u, reason in skipped[:10]:
                    print(f" - {u} -> {reason}")

            print("\nè¼¸å‡º JSONï¼š\n")
            print(output_content)

            if args.save:
                timestamp = time.strftime('%Y%m%d_%H%M%S')
                fname = f"reports/raw_{timestamp}.json"
                try:
                    os.makedirs('reports', exist_ok=True)
                    with open(fname, 'w', encoding='utf-8') as f:
                        f.write(output_content)
                    print(f"ğŸ“ å·²å°‡åŸå§‹æ“·å–çµæœå­˜ç‚º: {fname}")
                except Exception as e:
                    print(f"âŒ å„²å­˜çµæœå¤±æ•—: {e}")
        else:
            output_content = json.dumps({'mode': 'KEYWORD', 'items': [], 'skipped_count': 0}, ensure_ascii=False)
            print("\nâš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç›¸é—œé€£çµã€‚")

    # ------------------
    # è¼¸å‡ºåˆ°çµ‚ç«¯æ©Ÿä¸¦è¤‡è£½åˆ°å‰ªè²¼ç°¿
    # ------------------
    print("\n" + "="*50)
    print("ã€ç³»çµ±é‹è¡Œå ±å‘Šæ‘˜è¦ã€‘")

    # æ‰“å°å ±å‘Šæ‘˜è¦
    if is_valid_url(user_input):
        if llm_report:
            print(f"åˆ†æç›®æ¨™: {user_input[:50]}...")
            print(f"æœ€çµ‚ç­‰ç´š: {llm_report.credibility_level}")
    else:
        print(f"é—œéµå­—: {user_input}")
        print(f"æ‰¹é‡åˆ†ææ–‡ç« æ•¸: {len(all_analyses)}")
        if all_analyses:
            print(f"å‰ 3 ç­†æœ€å¯ç–‘é€£çµ:")
            # æŒ‰åˆ†æ•¸æ’åºï¼Œé¸æ“‡æœ€ä½åˆ†çš„ä¸‰å€‹ (æœ€å¯ç–‘)
            top_suspicious = sorted(all_analyses, key=lambda x: x['ai_score'])[:3]
            for analysis in top_suspicious:
                print(f" Â - {analysis['domain']} ({analysis['ai_level']}) - {analysis['ai_score']:.3f}")

    print("="*50)

    # --- çµ±ä¸€å¯«å‡º JSON è¨˜éŒ„æª”ï¼ˆä¾ä½¿ç”¨è€…è¦æ±‚ï¼šæ¯æ¬¡éƒ½å¯«ï¼‰ ---
    try:
        os.makedirs('reports', exist_ok=True)
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        fname = f"reports/raw_{timestamp}.json"
        with open(fname, 'w', encoding='utf-8') as f:
            f.write(output_content)
        print(f"ğŸ“ å·²å°‡åŸå§‹æ“·å–çµæœå­˜ç‚º: {fname}")
    except Exception as e:
        print(f"âŒ è‡ªå‹•å„²å­˜ JSON å¤±æ•—: {e}")

    try:
        pyperclip.copy(output_content)
        print("ğŸ‰ å®Œæ•´å ±å‘Šå…§å®¹å·²è‡ªå‹•è¤‡è£½åˆ°æ‚¨çš„å‰ªè²¼ç°¿ä¸­ã€‚")
    except pyperclip.PyperclipException:
        print("âŒ ç„¡æ³•å­˜å–å‰ªè²¼ç°¿ã€‚è«‹æ‰‹å‹•è¤‡è£½ä¸Šæ–¹å…§å®¹ã€‚")
        
if __name__ == "__main__":
    run_analysis_system()